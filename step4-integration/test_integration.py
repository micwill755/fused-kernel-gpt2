#!/usr/bin/env python3
"""
Integration test suite for fused CUDA kernels in GPT-2.
"""

import torch
import torch.nn as nn
from transformers import GPT2Config, GPT2Model, GPT2Tokenizer
import numpy as np
import time
import sys
import os
import warnings

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from gpt2_model import OptimizedGPT2Model, OptimizedGPT2LMHeadModel
from autograd_function import FusedAttentionLayerNormModule

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')


class IntegrationTestSuite:
    """Comprehensive integration test suite."""
    
    def __init__(self, device='cuda'):
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.test_results = []
        
        # Test configurations
        self.small_config = GPT2Config(
            vocab_size=50257, n_positions=512, n_embd=768, 
            n_layer=4, n_head=12, n_inner=3072
        )
        
        print(f"Running tests on: {self.device}")
        if self.device == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name()}")
    
    def test_autograd_function(self):
        """Test the custom autograd function."""
        print("\nüß™ Testing Autograd Function...")
        
        batch_size, seq_len, hidden_size, num_heads = 2, 64, 768, 12
        
        # Create test data
        torch.manual_seed(42)
        input_tensor = torch.randn(batch_size, seq_len, hidden_size, 
                                 device=self.device, requires_grad=True)
        
        # Create fused module
        fused_module = FusedAttentionLayerNormModule(hidden_size, num_heads).to(self.device)
        
        # Forward pass
        output = fused_module(input_tensor)
        
        # Backward pass
        loss = output.sum()
        loss.backward()
        
        # Check gradients exist
        assert input_tensor.grad is not None, "Input gradients not computed"
        assert fused_module.q_weight.grad is not None, "Q weight gradients not computed"
        assert fused_module.ln_weight.grad is not None, "LayerNorm weight gradients not computed"
        
        print("  ‚úÖ Autograd function test passed")
        return True
    
    def test_model_creation(self):
        """Test optimized model creation."""
        print("\nüèóÔ∏è Testing Model Creation...")
        
        # Create optimized model
        model = OptimizedGPT2Model(self.small_config).to(self.device)
        
        # Check model structure
        assert len(model.h) == self.small_config.num_hidden_layers, "Wrong number of layers"
        
        # Check that blocks use fused attention
        for i, block in enumerate(model.h):
            assert hasattr(block.attn, 'fused_attn_ln'), f"Block {i} missing fused attention"
        
        # Test parameter count
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  Model parameters: {total_params:,}")
        
        print("  ‚úÖ Model creation test passed")
        return True
    
    def test_forward_pass(self):
        """Test forward pass through optimized model."""
        print("\n‚û°Ô∏è Testing Forward Pass...")
        
        model = OptimizedGPT2Model(self.small_config).to(self.device)
        model.eval()
        
        # Test different input sizes
        test_cases = [
            (1, 64),   # Single sequence
            (2, 128),  # Small batch
            (4, 256),  # Larger batch
        ]
        
        for batch_size, seq_len in test_cases:
            input_ids = torch.randint(0, self.small_config.vocab_size, 
                                    (batch_size, seq_len), device=self.device)
            
            with torch.no_grad():
                outputs = model(input_ids)
            
            expected_shape = (batch_size, seq_len, self.small_config.hidden_size)
            assert outputs.last_hidden_state.shape == expected_shape, \
                f"Wrong output shape: {outputs.last_hidden_state.shape} vs {expected_shape}"
            
            # Check for NaN or Inf
            assert not torch.isnan(outputs.last_hidden_state).any(), "NaN in output"
            assert not torch.isinf(outputs.last_hidden_state).any(), "Inf in output"
            
            print(f"  ‚úÖ Forward pass test passed for shape {batch_size}x{seq_len}")
        
        return True
    
    def test_backward_pass(self):
        """Test backward pass through optimized model."""
        print("\n‚¨ÖÔ∏è Testing Backward Pass...")
        
        model = OptimizedGPT2Model(self.small_config).to(self.device)
        model.train()
        
        batch_size, seq_len = 2, 64
        input_ids = torch.randint(0, self.small_config.vocab_size, 
                                (batch_size, seq_len), device=self.device)
        
        # Forward pass
        outputs = model(input_ids)
        
        # Compute loss and backward pass
        loss = outputs.last_hidden_state.sum()
        loss.backward()
        
        # Check that gradients exist for all parameters
        for name, param in model.named_parameters():
            if param.requires_grad:
                assert param.grad is not None, f"No gradient for {name}"
                assert not torch.isnan(param.grad).any(), f"NaN gradient for {name}"
                assert not torch.isinf(param.grad).any(), f"Inf gradient for {name}"
        
        print("  ‚úÖ Backward pass test passed")
        return True
    
    def test_numerical_accuracy(self):
        """Test numerical accuracy against reference implementation."""
        print("\nüî¢ Testing Numerical Accuracy...")
        
        # Create models with same weights
        standard_model = GPT2Model(self.small_config).to(self.device)
        optimized_model = OptimizedGPT2Model(self.small_config).to(self.device)
        
        # Copy weights (simplified - in practice would need careful weight mapping)
        # For this test, we'll just check that outputs are reasonable
        
        batch_size, seq_len = 2, 64
        input_ids = torch.randint(0, self.small_config.vocab_size, 
                                (batch_size, seq_len), device=self.device)
        
        with torch.no_grad():
            standard_output = standard_model(input_ids).last_hidden_state
            optimized_output = optimized_model(input_ids).last_hidden_state
        
        # Check output statistics are reasonable
        std_mean = standard_output.mean().item()
        std_std = standard_output.std().item()
        opt_mean = optimized_output.mean().item()
        opt_std = optimized_output.std().item()
        
        print(f"  Standard model - Mean: {std_mean:.6f}, Std: {std_std:.6f}")
        print(f"  Optimized model - Mean: {opt_mean:.6f}, Std: {opt_std:.6f}")
        
        # Check that outputs are in reasonable range
        assert abs(opt_mean) < 1.0, f"Output mean too large: {opt_mean}"
        assert 0.1 < opt_std < 2.0, f"Output std out of range: {opt_std}"
        
        print("  ‚úÖ Numerical accuracy test passed")
        return True
    
    def test_memory_efficiency(self):
        """Test memory efficiency of optimized model."""
        print("\nüíæ Testing Memory Efficiency...")
        
        if self.device == 'cpu':
            print("  ‚è≠Ô∏è Skipping memory test on CPU")
            return True
        
        batch_size, seq_len = 4, 256
        input_ids = torch.randint(0, self.small_config.vocab_size, 
                                (batch_size, seq_len), device=self.device)
        
        # Test standard model memory usage
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        standard_model = GPT2Model(self.small_config).to(self.device)
        with torch.no_grad():
            _ = standard_model(input_ids)
        
        standard_memory = torch.cuda.max_memory_allocated()
        del standard_model
        
        # Test optimized model memory usage
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        optimized_model = OptimizedGPT2Model(self.small_config).to(self.device)
        with torch.no_grad():
            _ = optimized_model(input_ids)
        
        optimized_memory = torch.cuda.max_memory_allocated()
        del optimized_model
        
        memory_savings = (standard_memory - optimized_memory) / standard_memory * 100
        
        print(f"  Standard model memory: {standard_memory / 1024**2:.1f} MB")
        print(f"  Optimized model memory: {optimized_memory / 1024**2:.1f} MB")
        print(f"  Memory savings: {memory_savings:.1f}%")
        
        # Note: Memory savings might be minimal or even negative in this test
        # because we're not actually using the fused kernels (they're not compiled)
        # This is more of a structural test
        
        print("  ‚úÖ Memory efficiency test completed")
        return True
    
    def test_performance_scaling(self):
        """Test performance scaling with different input sizes."""
        print("\n‚ö° Testing Performance Scaling...")
        
        model = OptimizedGPT2Model(self.small_config).to(self.device)
        model.eval()
        
        test_sizes = [
            (1, 64),
            (1, 128),
            (1, 256),
            (2, 128),
            (4, 64),
        ]
        
        results = []
        
        for batch_size, seq_len in test_sizes:
            input_ids = torch.randint(0, self.small_config.vocab_size, 
                                    (batch_size, seq_len), device=self.device)
            
            # Warm up
            for _ in range(5):
                with torch.no_grad():
                    _ = model(input_ids)
            
            if self.device == 'cuda':
                torch.cuda.synchronize()
            
            # Time execution
            start_time = time.time()
            num_iterations = 20
            
            for _ in range(num_iterations):
                with torch.no_grad():
                    _ = model(input_ids)
            
            if self.device == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            avg_time = (end_time - start_time) / num_iterations * 1000  # ms
            
            tokens_per_sec = (batch_size * seq_len) / (avg_time / 1000)
            
            results.append({
                'batch_size': batch_size,
                'seq_len': seq_len,
                'time_ms': avg_time,
                'tokens_per_sec': tokens_per_sec
            })
            
            print(f"  {batch_size}x{seq_len}: {avg_time:.2f}ms, {tokens_per_sec:.0f} tokens/sec")
        
        # Check that performance scales reasonably
        # Larger inputs should have better tokens/sec (up to memory limits)
        print("  ‚úÖ Performance scaling test completed")
        return True
    
    def test_language_model_integration(self):
        """Test integration with language modeling head."""
        print("\nüó£Ô∏è Testing Language Model Integration...")
        
        lm_model = OptimizedGPT2LMHeadModel(self.small_config).to(self.device)
        lm_model.eval()
        
        batch_size, seq_len = 2, 64
        input_ids = torch.randint(0, self.small_config.vocab_size, 
                                (batch_size, seq_len), device=self.device)
        
        with torch.no_grad():
            outputs = lm_model(input_ids)
        
        # Check output shape
        expected_shape = (batch_size, seq_len, self.small_config.vocab_size)
        assert outputs.logits.shape == expected_shape, \
            f"Wrong logits shape: {outputs.logits.shape} vs {expected_shape}"
        
        # Check that logits are reasonable
        assert not torch.isnan(outputs.logits).any(), "NaN in logits"
        assert not torch.isinf(outputs.logits).any(), "Inf in logits"
        
        # Test text generation (simple)
        generated = lm_model.generate(input_ids[:1, :10], max_length=20, do_sample=False)
        assert generated.shape[1] == 20, f"Wrong generated length: {generated.shape[1]}"
        
        print("  ‚úÖ Language model integration test passed")
        return True
    
    def test_gradient_accumulation(self):
        """Test gradient accumulation compatibility."""
        print("\nüìà Testing Gradient Accumulation...")
        
        model = OptimizedGPT2Model(self.small_config).to(self.device)
        model.train()
        
        batch_size, seq_len = 2, 64
        accumulation_steps = 4
        
        # Clear gradients
        model.zero_grad()
        
        total_loss = 0
        for step in range(accumulation_steps):
            input_ids = torch.randint(0, self.small_config.vocab_size, 
                                    (batch_size, seq_len), device=self.device)
            
            outputs = model(input_ids)
            loss = outputs.last_hidden_state.sum() / accumulation_steps
            loss.backward()
            total_loss += loss.item()
        
        # Check that gradients accumulated
        grad_norm = 0
        for param in model.parameters():
            if param.grad is not None:
                grad_norm += param.grad.norm().item() ** 2
        grad_norm = grad_norm ** 0.5
        
        assert grad_norm > 0, "No gradients accumulated"
        print(f"  Accumulated gradient norm: {grad_norm:.6f}")
        
        print("  ‚úÖ Gradient accumulation test passed")
        return True
    
    def run_all_tests(self):
        """Run all integration tests."""
        print("üöÄ Starting Integration Test Suite")
        print("=" * 50)
        
        tests = [
            ("Autograd Function", self.test_autograd_function),
            ("Model Creation", self.test_model_creation),
            ("Forward Pass", self.test_forward_pass),
            ("Backward Pass", self.test_backward_pass),
            ("Numerical Accuracy", self.test_numerical_accuracy),
            ("Memory Efficiency", self.test_memory_efficiency),
            ("Performance Scaling", self.test_performance_scaling),
            ("Language Model Integration", self.test_language_model_integration),
            ("Gradient Accumulation", self.test_gradient_accumulation),
        ]
        
        passed_tests = 0
        failed_tests = []
        
        for test_name, test_func in tests:
            try:
                success = test_func()
                if success:
                    passed_tests += 1
                else:
                    failed_tests.append(test_name)
            except Exception as e:
                print(f"  ‚ùå {test_name} failed with error: {e}")
                failed_tests.append(test_name)
                import traceback
                traceback.print_exc()
        
        # Print summary
        print(f"\nüìä Test Summary")
        print("=" * 30)
        print(f"Total tests: {len(tests)}")
        print(f"Passed: {passed_tests}")
        print(f"Failed: {len(failed_tests)}")
        
        if failed_tests:
            print(f"Failed tests: {', '.join(failed_tests)}")
        
        if len(failed_tests) == 0:
            print("\nüéâ All integration tests passed!")
            return True
        else:
            print(f"\nüí• {len(failed_tests)} tests failed")
            return False


def main():
    """Main test function."""
    # Check CUDA availability
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"Using CUDA device: {torch.cuda.get_device_name()}")
    else:
        device = 'cpu'
        print("CUDA not available, using CPU")
    
    # Run integration tests
    test_suite = IntegrationTestSuite(device=device)
    success = test_suite.run_all_tests()
    
    if success:
        print("\n‚úÖ Integration test suite completed successfully!")
        print("\nüéØ Next Steps:")
        print("1. Build the CUDA kernels: cd ../step3-fused && python setup.py build_ext --inplace")
        print("2. Run performance benchmarks: python benchmark.py")
        print("3. Try the demo: python demo.py")
    else:
        print("\n‚ùå Some integration tests failed")
        print("Please check the error messages above and fix the issues")
    
    return success


if __name__ == "__main__":
    main()
